# Trace Search Fix - Summary

## Problem
The "Trace Search" panel in the End-to-End Tracing Dashboard was not working, while "Service Dependency Map" and "Application Logs" panels were functioning correctly.

## Root Causes Identified

### 1. **Missing `search_enabled` in Tempo Configuration** (PRIMARY ISSUE)
**File:** `otel-collector/tempo.yml`

Tempo's search API was disabled by default. The configuration was missing the critical `search_enabled: true` setting.

**Why the other panels worked:**
- **Service Dependency Map**: Uses metrics generated by Tempo's `metrics_generator` (service-graphs, span-metrics) which doesn't require search API
- **Application Logs**: Uses Loki datasource, completely independent of Tempo search

### 2. **Suboptimal Dashboard Query Configuration** (SECONDARY ISSUE)
**File:** `grafana/dashboards/end-to-end-tracing.json`

The Trace Search panel was using:
```json
"queryType": "traceql",
"query": "{}"
```

An empty TraceQL query `"{}"` may not return results reliably. Better to use the dedicated search query type.

## Fixes Applied

### Fix #1: Enable Tempo Search API
**File:** `otel-collector/tempo.yml:4-5`

```yaml
server:
  http_listen_port: 3200

# Enable search API for Grafana Trace Search functionality
search_enabled: true

distributor:
  receivers:
    ...
```

### Fix #2: Update Dashboard Panel Query Type
**File:** `grafana/dashboards/end-to-end-tracing.json:20-26`

**Before:**
```json
"targets": [
  {
    "queryType": "traceql",
    "refId": "A",
    "query": "{}"
  }
]
```

**After:**
```json
"targets": [
  {
    "queryType": "traceqlSearch",
    "refId": "A",
    "limit": 20,
    "tableType": "traces"
  }
]
```

## What Was Already Correct

✅ **Grafana Datasource Configuration** (`grafana/provisioning/datasources/datasources.yml:25`)
- Tempo datasource has stable UID: `uid: tempo`
- Search is not hidden: `search.hide: false`

✅ **Dashboard Datasource Reference** (`grafana/dashboards/end-to-end-tracing.json:12`)
- Correctly references: `"datasource": {"type": "tempo", "uid": "tempo"}`

✅ **Tempo Metrics Generator** (enables Service Dependency Map)
- Configured in `tempo.yml:36-48`

## Deployment & Validation

### Step 1: Restart the Stack
```bash
# If using startup script
./start-lab.sh

# Or manually with docker compose
docker compose -p lab down
docker compose -p lab up -d --build
```

### Step 2: Generate Traffic
Visit `http://192.168.122.250:8080` (or `http://localhost:8080` locally) and:
1. Create several tasks
2. Toggle task completion status
3. Delete some tasks

This generates traces across browser → backend → database.

### Step 3: Run Validation Script
```bash
./validate-trace-search.sh
```

This script will:
- Check container status (Tempo, Grafana)
- Verify `search_enabled: true` in tempo.yml
- Test Tempo health endpoint
- Query Tempo search API directly
- Validate Grafana datasource configuration
- Check dashboard panel configuration

### Step 4: Test in Grafana UI
1. Open: `http://192.168.122.250:3000/d/end-to-end-tracing/`
2. View the "Trace Search" panel (top panel)
3. You should see recent traces listed with:
   - Trace ID
   - Service Name (flask-backend, frontend)
   - Span count
   - Duration
   - Timestamp

### Step 5: Manual Tempo API Test (Optional)
```bash
# Test Tempo search API directly
curl -s 'http://192.168.122.250:3200/api/search?serviceName=flask-backend&minDuration=0ms&limit=5' | jq

# Expected output:
# {
#   "traces": [
#     {
#       "traceID": "abc123...",
#       "rootServiceName": "flask-backend",
#       "rootTraceName": "GET /api/tasks",
#       "startTimeUnixNano": "...",
#       "durationMs": 45
#     },
#     ...
#   ]
# }
```

## Troubleshooting

### If Trace Search is still empty after fixes:

#### 1. Check Tempo logs for errors
```bash
docker compose -p lab logs tempo --tail 50
```

Look for:
- "search_enabled" confirmation on startup
- Errors related to storage or indexing
- GRPC/OTLP receiver errors

#### 2. Verify traces are being ingested
```bash
# Check OTel Collector logs
docker compose -p lab logs otel-collector --tail 50

# Look for lines like:
# "Traces" {"#spans": 10}
```

#### 3. Check Grafana panel configuration manually
1. Go to dashboard: `http://192.168.122.250:3000/d/end-to-end-tracing/`
2. Click "Trace Search" panel title → Edit
3. Verify:
   - **Data source**: Tempo (not default, not Prometheus)
   - **Query inspector** shows queryType: "traceqlSearch" or "search"
   - **Time range**: "Last 1 hour" or "Last 15 minutes"
4. Try manual TraceQL query in panel editor:
   ```
   { service.name = "flask-backend" }
   ```

#### 4. Verify search tags are indexed
Tempo only indexes certain attributes by default. Check if your spans have `service.name`:

```bash
# In backend container
docker compose -p lab exec backend python -c "
from opentelemetry import trace
tracer = trace.get_tracer(__name__)
with tracer.start_as_current_span('test') as span:
    print('Span attributes:', span.attributes)
"
```

#### 5. Check time range alignment
- Grafana dashboard time range: Top-right corner (should be "Last 1 hour")
- Tempo only keeps traces for `block_retention: 1h` (see tempo.yml:16)
- If you generated traces > 1 hour ago, they may be deleted

### If validation script reports errors:

#### Error: "Tempo container is NOT running"
```bash
docker compose -p lab up -d tempo
docker compose -p lab logs tempo
```

#### Error: "search_enabled is NOT enabled"
Re-apply Fix #1 above, then restart:
```bash
docker compose -p lab restart tempo
```

#### Error: "Tempo search API returned 0 traces"
This is normal if:
1. No traffic has been generated yet → Visit `http://192.168.122.250:8080`
2. Traces are too old → Generate fresh traffic
3. Backend isn't sending traces → Check `docker compose -p lab logs backend`

## Testing the Full Pipeline

### End-to-End Test Sequence:
1. **Generate a trace** (visit frontend, create task)
2. **Verify in Tempo** (curl search API)
3. **View in Grafana Trace Search** (dashboard panel)
4. **Click a trace** → Opens detailed trace view
5. **Click "Logs for this span"** → Jumps to correlated logs in Loki

## Key Learnings

### Why Service Dependency Map worked but Trace Search didn't:
- **Service Map** → Powered by Tempo's `metrics_generator` which creates Prometheus metrics (`service-graphs`)
- **Trace Search** → Powered by Tempo's `/api/search` endpoint which requires `search_enabled: true`
- These are **separate subsystems** in Tempo. Ingestion can work fine while search is disabled.

### Grafana Panel Query Types for Tempo:
- **`search`**: Legacy search interface (Grafana < 9.x)
- **`traceqlSearch`**: Modern search with TraceQL filtering (Grafana 9.x+)
- **`traceql`**: Raw TraceQL query (requires valid query like `{service.name="foo"}`)
- **`serviceMap`**: Service dependency graph
- **`nativeSearch`**: Tempo-native search (older versions)

### Recommended Configuration Pattern:
For a "show all recent traces" panel:
```json
{
  "queryType": "traceqlSearch",
  "limit": 20,
  "tableType": "traces"
}
```

For filtered searches:
```json
{
  "queryType": "traceql",
  "query": "{ service.name = \"flask-backend\" && duration > 100ms }"
}
```

## References

- GPT's Analysis: Correctly identified `search_enabled` and query type issues
- Tempo Documentation: https://grafana.com/docs/tempo/latest/configuration/
- TraceQL Docs: https://grafana.com/docs/tempo/latest/traceql/
- Grafana Tempo Datasource: https://grafana.com/docs/grafana/latest/datasources/tempo/

## File Changes Summary

| File | Lines Changed | Description |
|------|---------------|-------------|
| `otel-collector/tempo.yml` | 4-5 | Added `search_enabled: true` |
| `grafana/dashboards/end-to-end-tracing.json` | 20-26 | Changed queryType from `traceql` to `traceqlSearch` |
| `validate-trace-search.sh` | New file | Automated validation script |

## Status
✅ **Fixed** - Trace Search should now work after restarting the stack and generating traffic.

---

**Last Updated:** 2025-10-20
**Related Issues:** Trace Search panel not displaying results
**Impact:** High (core observability feature)
**Difficulty:** Medium (required config analysis + Tempo internals knowledge)
