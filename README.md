## ğŸš€ Milestone 1 â€” Wrap-Up _(Oct 20, 2025)_

> **TL;DR** â€” A production-inspired observability lab: **Flask backend**, **browser-instrumented frontend**, **Nginx** reverse proxy, and a full **OTel â†’ Collector â†’ Tempo/Loki** pipeline with **Prometheus** metrics. Itâ€™s resilient to container/IP churn, exposes clear **SLIs**, and ships **end-to-end traces** with **correlated logs**.

---

### â­ Highlights

- **Three Pillars, Clean Separation**  
  Traces (**OTel â†’ Tempo**), Logs (**JSON â†’ Loki**), Metrics (**Prometheus client only** to prevent duplication).
- **Reverse Proxy Resilience**  
  Nginx dynamic DNS + variable `proxy_pass` fixed the 502/DNS race on container restart.
- **End-to-End Tracing**  
  Browser â†’ Flask â†’ DB spans with `trace_id`/`span_id` log correlation.
- **SLI/SLO Dashboards**  
  Availability, error rate, P95 latency, request rate, DB timings in Grafana.
- **Repeatable Ops**  
  Deployment Verification checklist ensures scrape targets, traces, and dashboards are actually live.

---

### ğŸ”§ Key Decisions & Fixes (at a glance)

| Area | Decision | Why | Outcome |
|---|---|---|---|
| **Metrics** | Prometheus client (not OTel metrics) | Avoid duplicate series / confusion | One source of truth for SLIs |
| **API Pathing** | Keep `/api` prefix via Nginx â†’ Flask | Transparent routing, simpler config | Fewer edge-cases & rewrites |
| **Proxy Resilience** | Dynamic DNS + `$upstream` `proxy_pass` | Backends get new IPs on restart | No more 502s on container churn |
| **Security** | SSH key-only auth + locked password | Production-grade baseline | Safer CI/CD deploy path |
| **Delivery** | Jenkins controller + Docker agent; SSH + `rsync` | Match bind-mount paths on target | Reliable, environment-aware deploys |

---

### ğŸ“Š What I Can Prove Works

- **Traces** show Browser â†’ Flask â†’ DB with parent/child spans.
- **Logs** include `trace_id`/`span_id` and correlate to traces.
- **Metrics** power **SLI/SLO** panels (availability, P95 latency).
- **Grafana** dashboards render live with traffic.
- **Nginx** survives container restarts without 502s.

> _Tip:_ Generate traffic, then open **Grafana â†’ SLI/SLO Dashboard** to watch metrics, traces, and logs populate.

---

### ğŸ“š Documentation Updated

**ARCHITECTURE.md**, **DESIGN-DECISIONS.md**, **IMPLEMENTATION-GUIDE.md**, **JOURNEY.md** â€” all aligned to first-person narrative, with troubleshooting and PromQL/LogQL tips for day-2 ops.

---

### ğŸ›£ï¸ Whatâ€™s Next

- **Phase 2** â†’ OPA/Rego policies, SAST/DAST, artifact management.  
- **Phase 3** â†’ Kubernetes (Helm, StatefulSets), PostgreSQL, possible service mesh.

> **Verification:** Run the **Deployment Verification** checklist after any change to confirm green scrapes, traces, and dashboards.


# Production-Grade Observability: From On-Premises to Cloud

## A Proof of Concept for Real-World SRE Practice

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Docker](https://img.shields.io/badge/Docker-Compose-2496ED?logo=docker)](https://www.docker.com/)
[![OpenTelemetry](https://img.shields.io/badge/OpenTelemetry-Enabled-blue?logo=opentelemetry)](https://opentelemetry.io/)
[![Grafana](https://img.shields.io/badge/Grafana-Dashboards-F46800?logo=grafana)](https://grafana.com/)
[![Prometheus](https://img.shields.io/badge/Prometheus-Metrics-E6522C?logo=prometheus)](https://prometheus.io/)

---

## ğŸ“– Table of Contents

- [Overview](#overview)
- [What Makes This Different](#what-makes-this-different)
- [Quick Start](#quick-start)
- [Documentation](#documentation)
- [Architecture](#architecture)
- [Technology Stack](#technology-stack)
- [Learning Outcomes](#learning-outcomes)
- [Roadmap](#roadmap)
- [Contributing](#contributing)

---

## Overview

This project is a **production-grade observability stack** built on simulated on-premises infrastructure. It demonstrates end-to-end distributed tracing, metrics collection, and log aggregation using industry-standard tools.

**But it's more than that.**

This is **milestone 1** of the **On-Prem Domain**â€”a comprehensive learning path that builds infrastructure expertise from bare metal to hybrid cloud. Every component was built through trial, error, and iteration, with all failures documented for learning.

### The Three Pillars of Observability

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TRACES (OpenTelemetry)                                      â”‚
â”‚  â€¢ Browser â†’ Backend â†’ Database correlation                  â”‚
â”‚  â€¢ Parent-child span relationships                           â”‚
â”‚  â€¢ W3C Trace Context propagation                             â”‚
â”‚  â€¢ Export: OTLP â†’ Tempo                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  METRICS (Prometheus)                                        â”‚
â”‚  â€¢ Request rates, error rates, latency percentiles           â”‚
â”‚  â€¢ SLI/SLO dashboards (availability, P95 latency)            â”‚
â”‚  â€¢ Database query performance tracking                       â”‚
â”‚  â€¢ Export: /metrics â†’ Prometheus scrape                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LOGS (Loki via OpenTelemetry)                               â”‚
â”‚  â€¢ Structured JSON logging                                   â”‚
â”‚  â€¢ Automatic trace_id/span_id injection                      â”‚
â”‚  â€¢ Log-to-trace correlation                                  â”‚
â”‚  â€¢ Export: OTLP â†’ Loki                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Application Stack

- **Frontend:** Nginx reverse proxy serving static HTML/JS
- **Backend:** Flask (Python 3.11) with full OpenTelemetry instrumentation
- **Database:** SQLite (migrating to PostgreSQL in Phase 3)
- **Observability:** OpenTelemetry Collector, Tempo, Prometheus, Loki, Grafana

---

## What Makes This Different

### ğŸ—ï¸ Built on Simulated On-Premises Infrastructure

Not cloud. Not managed services. **Bare metal virtualization** using KVM/QEMU/libvirt on Debian 13.

**Why?**
- Understand infrastructure from the ground up
- Learn what cloud abstractions hide (networking, storage, orchestration)
- Zero recurring costs (one-time hardware investment)
- Directly applicable to enterprise on-prem environments

### ğŸ”„ Deployed via CI/CD Pipeline

Automated deployment through **containerized Jenkins** with Docker agents, SSH-based deployment, and HashiCorp Vault for secrets management.

**Pipeline Features:**
- Git-based source control
- Automated builds with Docker Buildkit
- Remote deployment via SSH + rsync
- **Key-only SSH authentication** (ED25519, production-grade security)
- Health check validation
- Smoke tests

**Security First:**
- Password authentication disabled on all VMs
- Industry best practice implementation
- Future hardening planned: fail2ban, UFW firewall, 2FA

See: [ARCHITECTURE.md - CI/CD Pipeline Architecture](ARCHITECTURE.md#cicd-pipeline-architecture)

### ğŸ“Š Production-Ready Observability

Not a toy demo. This stack implements:
- **SLI/SLO tracking** (service availability >99%, P95 latency <500ms)
- **Distributed tracing** across all tiers (browser â†’ backend â†’ database)
- **Trace-log correlation** (click trace_id in logs â†’ jump to full trace)
- **Pre-built Grafana dashboards** (SLI/SLO, end-to-end tracing)
- **Error budget visualization** (track SLO compliance)

### ğŸ“š Comprehensive Documentation

Over **100,000 words** of documentation covering:
- **ARCHITECTURE.md:** Complete system design from hypervisor to application (47,000+ words)
- **DESIGN-DECISIONS.md:** All architectural choices, trade-offs, and rationale
- **JOURNEY.md:** The story of building this (struggles, breakthroughs, lessons)
- **IMPLEMENTATION-GUIDE.md:** Technical deep-dive with troubleshooting
- **docs/deployment-verification.md:** Step-by-step deployment validation

### ğŸ§ª Battle-Tested

Every error message was encountered, debugged, and documented:
- "Working outside of application context" (Flask lifecycle)
- "502 Bad Gateway" (Nginx DNS caching)
- "Database not found" (container filesystems)
- "Metric duplication" (OTel SDK vs. Prometheus client)

See: [JOURNEY.md](JOURNEY.md) for the complete story.

---

## Quick Start

### Prerequisites

- **Docker Engine** 20.10+ and **Docker Compose** 2.0+
- **4GB+ RAM** available
- **Ports available:** 3000, 3100, 3200, 4317, 4318, 5000, 8080, 9090

### Deployment Options

Choose the deployment method that fits your learning goals:

---

#### Option 1: Startup Script (Recommended - Aligned with Jenkins Pipeline)

**Why this option?** Follows the same deployment pattern as the production Jenkins pipeline, using project naming for better container organization.

```bash
# Clone repository
git clone https://github.com/illusivegit/Opentelemetry_Observability_Lab.git
cd Opentelemetry_Observability_Lab

# Run startup script
chmod +x start-lab.sh
./start-lab.sh
```

**What it does:**
- Defaults `PROJECT="lab"` for organized container naming (matches Jenkins pipeline) while honoring any `PROJECT` value you export before running the script
- Runs: `docker compose -p lab up -d --build`
- Validates all service health checks
- Provides clear status output

**Managing containers with project name:**

```bash
# View logs
docker compose -p lab logs -f backend

# Stop all services
docker compose -p lab down

# Restart a specific service
docker compose -p lab restart backend

# Check status
docker compose -p lab ps
```

**Design Decision:** Using `-p lab` creates a namespace for all containers (e.g., `lab-backend-1`, `lab-frontend-1`), preventing naming conflicts with other Docker Compose projects and matching the production pipeline pattern. See [DESIGN-DECISIONS.md](DESIGN-DECISIONS.md) for full rationale.

---

#### Option 2: Manual Docker Compose with Project Name (Same Standard as Pipeline)

If you prefer manual control but want to follow the same standard:

```bash
# Clone repository
git clone https://github.com/illusivegit/Opentelemetry_Observability_Lab.git
cd Opentelemetry_Observability_Lab

# Start with project name (matches Jenkins pipeline and startup script)
export DOCKER_BUILDKIT=1
docker compose -p lab up -d --build

# Check status
docker compose -p lab ps

# View logs
docker compose -p lab logs -f
```

**Note:** You **must** include `-p lab` in all subsequent commands (logs, down, restart) for proper container management.

---

#### Option 3: Simple Docker Compose (No Project Name)

If you prefer the simplest approach without project naming:

```bash
# Clone repository
git clone https://github.com/illusivegit/Opentelemetry_Observability_Lab.git
cd Opentelemetry_Observability_Lab

# Start all services (uses directory name as project)
docker compose up -d

# Check status
docker compose ps
```

**Trade-off:** Container names will use the directory name as prefix (e.g., `opentelemetry_observability_lab-backend-1`). This works fine but doesn't match the standardized pipeline pattern.

---

#### Option 4: Jenkins Pipeline Deployment (Full CI/CD Experience)

**For the complete production deployment experience:**

See: [ARCHITECTURE.md - CI/CD Pipeline Architecture](ARCHITECTURE.md#cicd-pipeline-architecture)

1. Set up Jenkins controller + Docker agent (see docs)
2. Configure VM target (SSH keys, Docker daemon)
3. Run pipeline: Checkout â†’ Sync â†’ Deploy â†’ Smoke Tests

**Pipeline deploys using:**
```bash
ssh ${VM_USER}@${VM_IP} "
  cd ${VM_DIR} && \
  PROJECT=lab ./start-lab.sh   # Override as needed (e.g., PROJECT=staging)
"
```

---

### Verify Deployment

**Important:** Replace `localhost` with your VM's IP address if deploying to a virtual machine.

**For VM deployments:** If you deployed to a VM (e.g., via Jenkins pipeline or manual SSH), replace `localhost` with your VM's IP address:
- Example: `http://192.168.122.250:8080` instead of `http://localhost:8080`

**Application:**
- Frontend: `http://<VM_IP>:8080` (or `http://localhost:8080` for local)
- Backend API: `http://<VM_IP>:5000/api/tasks`

**Observability Stack:**
- Grafana: `http://<VM_IP>:3000` (anonymous login enabled)
- Prometheus: `http://<VM_IP>:9090`
- Tempo: `http://<VM_IP>:3200/ready`
- Loki: `http://<VM_IP>:3100/ready`

**Generate Traffic:**
1. Open `http://<VM_IP>:8080` (use your VM's IP or `localhost`)
2. Create a few tasks (click "Add Task")
3. Navigate to Grafana â†’ Dashboards â†’ "SLI/SLO Dashboard - Task Manager"
4. See metrics, traces, and logs populate

**Detailed Verification Guide:** [docs/deployment-verification.md](docs/deployment-verification.md)

---

## Documentation

### Core Documents

| Document | Purpose | Pages |
|----------|---------|-------|
| **[ARCHITECTURE.md](ARCHITECTURE.md)** | Complete system architecture from infrastructure to application | 47,000 words |
| **[DESIGN-DECISIONS.md](DESIGN-DECISIONS.md)** | All architectural decisions with trade-offs and rationale | 15,000 words |
| **[JOURNEY.md](JOURNEY.md)** | The story of building this (failures, breakthroughs, lessons) | 12,000 words |
| **[IMPLEMENTATION-GUIDE.md](IMPLEMENTATION-GUIDE.md)** | Technical deep-dive with troubleshooting scenarios | 47,000 words |
| **[docs/deployment-verification.md](docs/deployment-verification.md)** | Step-by-step post-deployment validation | 4,000 words |
| **[docs/nginx-proxy-pass-options.md](docs/nginx-proxy-pass-options.md)** | Nginx reverse proxy design (proxy vs. CORS) | 1,500 words |

**Total:** 125,000+ words of comprehensive documentation

### Quick References

**Common Commands:**
```bash
# View logs
docker compose logs -f backend
docker compose logs -f otel-collector

# Restart specific service
docker compose restart backend

# Check service health
curl http://localhost:13133  # OTEL Collector healthcheck
curl http://localhost:5000/health  # Backend health

# Tear down stack
docker compose down

# Tear down with volume cleanup (fresh start)
docker compose down -v
```

**Grafana Dashboards:**
- **SLI/SLO Dashboard:** Service availability, P95 latency, error rates, DB performance
- **End-to-End Tracing:** Distributed trace visualization, service dependency maps

**Prometheus Queries (SLIs):**
```promql
# Service Availability
100 * (1 - (sum(rate(http_errors_total[5m])) / sum(rate(http_requests_total[5m]))))

# P95 Response Time
histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le))

# Database P95 Latency
histogram_quantile(0.95, sum(rate(db_query_duration_seconds_bucket[5m])) by (le, operation))
```

**Loki Log Queries:**
```logql
# All logs from backend
{service_name="flask-backend"}

# Filter by log level
{service_name="flask-backend", level="ERROR"}

# Search for specific trace
{service_name="flask-backend"} | json | trace_id="a1b2c3d4e5f6789..."
```

**Tempo Trace Queries (TraceQL):**
```traceql
# Find slow requests (>500ms)
{duration > 500ms}

# Find errors in specific endpoint
{span.http.route = "/api/tasks" && status = error}

# Find slow database queries
{span.db.query.duration > 50ms}
```

---

## Architecture

### High-Level Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   PHYSICAL HOST (Debian 13)                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚            KVM/QEMU/Libvirt Hypervisor                    â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚  VM: 192.168.122.250 (Application VM)               â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â€¢ Docker Engine                                     â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â€¢ Observability Stack (7 containers)                â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚  VM: 192.168.122.x (Jenkins VM)                     â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â€¢ Jenkins Controller                                â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â€¢ Docker Agents                                     â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â€¢ HashiCorp Vault                                   â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Container Architecture (Application VM)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Docker Network: otel-network (bridge)                          â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Frontend    â”‚      â”‚  Backend     â”‚      â”‚  OTel        â”‚  â”‚
â”‚  â”‚  (Nginx)     â”‚â”€â”€â”€â”€â”€â–¶â”‚  (Flask)     â”‚â”€â”€â”€â”€â”€â–¶â”‚  Collector   â”‚  â”‚
â”‚  â”‚  Port: 8080  â”‚      â”‚  Port: 5000  â”‚      â”‚  Port: 4318  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                               â”‚                     â”‚          â”‚
â”‚                               â–¼                     â–¼          â”‚
â”‚                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚                        â”‚  SQLite      â”‚      â”‚  Tempo       â”‚  â”‚
â”‚                        â”‚  Database    â”‚      â”‚  (Traces)    â”‚  â”‚
â”‚                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚  Prometheus  â”‚  â”‚
â”‚                                              â”‚  (Metrics)   â”‚  â”‚
â”‚                                              â”‚  Loki (Logs) â”‚  â”‚
â”‚                                              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                     â”‚          â”‚
â”‚                                                     â–¼          â”‚
â”‚                                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚                                              â”‚  Grafana     â”‚  â”‚
â”‚                                              â”‚  Port: 3000  â”‚  â”‚
â”‚                                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Design Decisions:**

1. **Nginx Reverse Proxy** (not CORS headers) for `/api/*` routing
   - Same-origin requests eliminate preflight overhead
   - Production-standard architecture
   - See: [DESIGN-DECISIONS.md - DD-003](DESIGN-DECISIONS.md#dd-003-frontend-backend-communication-nginx-proxy-vs-cors-headers)

2. **Dynamic DNS Resolution** in Nginx (not static hostname)
   - Handles backend container restarts gracefully
   - No 502 errors on IP changes
   - See: [DESIGN-DECISIONS.md - DD-007](DESIGN-DECISIONS.md#dd-007-nginx-dns-resolution-static-vs-dynamic)

3. **Prometheus Client for Metrics** (not OTel SDK metrics)
   - Eliminates duplication (single source of truth)
   - OTel reserved for traces and logs (distributed context)
   - See: [DESIGN-DECISIONS.md - DD-006](DESIGN-DECISIONS.md#dd-006-metric-instrumentation-prometheus-client-vs-otel-sdk-metrics)

**Full Architecture Docs:** [ARCHITECTURE.md](ARCHITECTURE.md)

---

## Technology Stack

### Infrastructure Layer

| Component | Version | Purpose |
|-----------|---------|---------|
| **KVM/QEMU** | Native (kernel 6.12) | Hypervisor (hardware virtualization) |
| **libvirt** | Latest | VM lifecycle management, virtual networking |
| **Debian** | 13 (Trixie) | Host and guest OS |

### Application Layer

| Component | Version | Purpose |
|-----------|---------|---------|
| **Python** | 3.11 | Backend runtime |
| **Flask** | 3.0.0 | Web framework |
| **SQLAlchemy** | 3.1.1 | ORM (migrating to PostgreSQL in Phase 3) |
| **Nginx** | Alpine (latest) | Reverse proxy, static file server |

### Observability Layer

| Component | Version | Purpose |
|-----------|---------|---------|
| **OpenTelemetry Collector** | 0.96.0 (contrib) | Telemetry pipeline hub |
| **OpenTelemetry Python SDK** | 1.22.0 | Auto-instrumentation (Flask, SQLAlchemy) |
| **Grafana Tempo** | 2.3.1 | Distributed trace storage (TraceQL) |
| **Prometheus** | 2.48.1 | Metrics storage (TSDB), PromQL queries |
| **Grafana Loki** | 2.9.3 | Log aggregation, LogQL queries |
| **Grafana** | 10.2.3 | Unified visualization, dashboards |
| **prometheus_client** | Latest | Python metrics library (Counters, Histograms) |

### CI/CD Layer

| Component | Version | Purpose |
|-----------|---------|---------|
| **Jenkins** | LTS (JDK 17) | CI/CD controller |
| **Docker** | 20.10+ | Containerization |
| **Docker Compose** | 2.0+ | Multi-container orchestration |
| **HashiCorp Vault** | Latest | Secrets management |
| **rsync** | 3.2.7 | File synchronization to VMs |

---

## Learning Outcomes

### What You'll Understand After This Project

**Infrastructure:**
- âœ… Hypervisor concepts (KVM, QEMU, hardware virtualization)
- âœ… Virtual networking (libvirt bridges, NAT, DNS/DHCP)
- âœ… Storage pools and virtual volumes (qcow2, LVM)
- âœ… VM lifecycle management (virsh, virt-manager, XML configs)

**Containerization:**
- âœ… Docker networking (bridge, service discovery, dynamic DNS)
- âœ… Healthchecks and startup ordering (`depends_on: service_healthy`)
- âœ… Bind mounts and volume management
- âœ… Multi-stage builds and image optimization

**Observability:**
- âœ… Distributed tracing (parent-child spans, W3C Trace Context)
- âœ… Metrics instrumentation (counters, histograms, percentiles)
- âœ… Structured logging with trace correlation
- âœ… SLI/SLO implementation (availability, latency, error budgets)
- âœ… PromQL, LogQL, and TraceQL query languages

**CI/CD:**
- âœ… Jenkins pipeline design (Groovy syntax, stages, agents)
- âœ… SSH-based deployment (rsync, sshagent, remote execution)
- âœ… Docker context management
- âœ… Secrets management (Vault integration)
- âœ… Pipeline smoke tests and health checks

**Security:**
- âœ… SSH key-based authentication (ED25519 key generation)
- âœ… Disabling password authentication (production hardening)
- âœ… User account security (password locking with `passwd -l`)
- âœ… SSH daemon hardening (`sshd_config` best practices)
- âœ… Security-first mindset (practice production rigor in lab)

**Web Architecture:**
- âœ… Reverse proxy configuration (Nginx dynamic DNS resolution)
- âœ… Flask application context lifecycle
- âœ… SQLAlchemy event listeners (performance tracking)
- âœ… CORS vs. same-origin requests (architectural trade-offs)

**Debugging Distributed Systems:**
- âœ… Trace data flow across network boundaries
- âœ… Interpret error messages (framework lifecycles, DNS resolution)
- âœ… When to restart vs. rebuild (Docker caching, DNS state)
- âœ… Use logs, metrics, and traces together for root cause analysis

---

## Roadmap

### Current State: Phase 1 (Complete) âœ…

- âœ… KVM/libvirt virtualization infrastructure
- âœ… Containerized Jenkins CI/CD pipeline
- âœ… Full observability stack (traces, metrics, logs)
- âœ… Automated deployment (SSH + rsync + docker compose)
- âœ… Pre-built Grafana dashboards (SLI/SLO)
- âœ… Comprehensive documentation (125,000+ words)

### Phase 2: Advanced CI/CD & Security (Next 3-6 Months)

**Pre-Commit Hooks:**
- Host IDE: `black`, `flake8`, `prettier`, `detect-secrets`
- GitHub: Branch protection, pre-receive hooks

**Policy as Code:**
- Learn Rego (OPA policy language)
- Implement Conftest CLI in pipeline
- Enforce: No root containers, resource limits, no hardcoded secrets

**Security Scanning Pipeline:**
- SonarQube (SAST - code quality, security vulnerabilities)
- Snyk (dependency scanning - Python, JavaScript)
- Trivy (container image scanning)
- JFrog Artifactory + Xray (artifact versioning, compliance)

**Quality Gates:**
- Pipeline fails on high/critical vulnerabilities
- Auto-create Jira tickets with remediation steps
- Slack notifications to #devsecops channel

**Server Hardening:**
- Implement comprehensive Linux hardening (fail2ban, UFW firewall, kernel tuning)
- Reference: [How To Secure A Linux Server](https://github.com/imthenachoman/How-To-Secure-A-Linux-Server)
- Incremental implementation: SSH keys (done) â†’ fail2ban â†’ firewall â†’ HIDS â†’ audit logging

### Phase 3: Kubernetes Migration (6-12 Months)

**Goals:**
- Migrate from Docker Compose â†’ Kubernetes manifests
- Create Helm charts for observability stack
- Replace SQLite â†’ PostgreSQL StatefulSet
- Deploy to on-prem K8s cluster (kubeadm on VMs)

**New Skills:**
- Kubernetes networking (Services, Ingress, NetworkPolicies)
- Persistent storage (PVCs, StorageClasses, dynamic provisioning)
- StatefulSets vs. Deployments
- Helm templating (values.yaml, chart dependencies)
- Service mesh integration (Istio/Linkerd for automatic observability)

### Phase 4: Hybrid Cloud (12-18 Months)

**Implement the 5 R's of Cloud Migration:**

1. **Rehost (Lift and Shift):** Deploy to AWS EKS / GCP GKE / Azure AKS
2. **Replatform (Lift, Tinker, Shift):** Use managed Prometheus, RDS/Cloud SQL
3. **Refactor (Re-architect):** Decompose to microservices, use cloud messaging
4. **Repurchase (SaaS):** Evaluate Datadog vs. self-hosted (cost/features)
5. **Relocate (Hypervisor Lift-and-Shift):** AWS MGN / Azure Migrate

**Hybrid Architecture:**
- Dev/test on-prem (KVM VMs)
- Prod in cloud (managed Kubernetes)
- Unified observability (Grafana queries both)

### Phase 5: Advanced Topics (18+ Months)

- Ansible automation (playbooks for VM provisioning, K8s deployment)
- Complex networking (BGP routing, multi-region, private service mesh)
- Bare metal self-hosting (Talos Linux, home lab with rack servers)

**Full Roadmap:** [ARCHITECTURE.md - Future Roadmap](ARCHITECTURE.md#future-roadmap)

---

## Contributing

This is a learning project, and contributions are welcome!

**Ways to Contribute:**
- ğŸ› **Report bugs** (open an issue with reproduction steps)
- ğŸ“– **Improve documentation** (fix typos, add clarifications)
- ğŸ’¡ **Suggest enhancements** (new features, architectural improvements)
- ğŸ¤ **Share your experience** (Did this help you learn? Tell me!)

**How to Contribute:**
1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

**Guidelines:**
- Follow existing code style (black for Python, prettier for JS)
- Update documentation for any architectural changes
- Add tests if adding new functionality
- Reference issues in commit messages (`Fixes #42`)

---

## Acknowledgments

**Inspired By:**
- Google SRE Book (Site Reliability Engineering principles)
- Observability Engineering (Charity Majors, Liz Fong-Jones, George Miranda)
- OpenTelemetry Documentation
- Production experiences from Netflix, Uber, Airbnb engineering blogs

**Community:**
- OpenTelemetry Slack (#opentelemetry)
- Grafana Community Forums
- r/devops, r/sre, r/kubernetes

---

## License

MIT License - See [LICENSE](LICENSE) for details.

**TL;DR:** Use this for your own learning. Modify it. Break it. Fix it. Share it. Just keep the attribution.

---

## Contact

**GitHub:** [illusivegit](https://github.com/illusivegit)
**Project:** [Opentelemetry_Observability_Lab](https://github.com/illusivegit/Opentelemetry_Observability_Lab)

**Questions? Issues? Feedback?**
- Open an issue on GitHub
- Check existing documentation (125,000+ words of answers)

---

## Final Thoughts

> "You don't learn observability by reading about it. You learn by building it, breaking it, and debugging it at 2 AM when nothing makes sense and then suddenlyâ€”**click**â€”everything is clear."

This project represents **hundreds of hours** of building, debugging, documenting, and learning. Every error message taught something. Every breakthrough revealed a deeper understanding.

If you're starting your own journey into DevSecOps, SRE, or observability engineering:

**Start small. Build something. Break it. Fix it. Document it.**

That's how you go from reading about distributed tracing to **understanding** how traces flow from browser â†’ backend â†’ database â†’ collector â†’ storage â†’ visualization.

**Happy Building.**

---

**Created:** October 2025
**Last Updated:** October 20, 2025
**Version:** 2.0 (Production-Ready Proof of Concept)
**Status:** âœ… Milestone 1 Complete | ğŸš§ Phase 2 Planning

**Documentation Stats:**
- ğŸ“„ 6 core documents
- ğŸ“ 125,000+ words
- ğŸ¯ 100% coverage (infrastructure â†’ application â†’ observability â†’ CI/CD)
- ğŸ“Š 15+ architecture diagrams
- ğŸ”§ 50+ design decisions documented

**Ready to Learn? Read:** [JOURNEY.md](JOURNEY.md) - Start here for the full story.
